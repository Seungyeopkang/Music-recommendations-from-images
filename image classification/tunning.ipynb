{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face 로그인\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = r'C:\\\\Users\\\\user\\\\Desktop\\\\jupyter notebook\\\\gg-project-main\\\\image classification\\\\clip_results.csv'\n",
    "\n",
    "# 파일 읽기 및 emotion별 데이터 개수 계산\n",
    "try:\n",
    "    # 파일 읽기 (인코딩 지정)\n",
    "    clip_data = pd.read_csv(file_path, encoding='cp949')  # CP949는 한글 Windows에서 일반적으로 사용\n",
    "    # emotion별 데이터 개수 계산\n",
    "    emotion_counts = clip_data['emotion'].value_counts()\n",
    "    \n",
    "    # 출력\n",
    "    print(\"Emotion Counts:\")\n",
    "    print(emotion_counts)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"인코딩 오류가 발생했습니다. 다른 인코딩 형식을 시도해 보세요 (예: 'ISO-8859-1').\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일 '{file_path}'이(가) 존재하지 않습니다. 파일 경로를 확인해주세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1단계: 먼저 Train과 Test를 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2단계: Train 데이터를 다시 Train과 Validation으로 분리\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.125, random_state=42, stratify=y_train\n",
    ")  # 0.125는 전체 데이터의 10%를 Validation으로 사용\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Train set label counts:\")\n",
    "print(y_train_final.value_counts())\n",
    "print(\"\\nValidation set label counts:\")\n",
    "print(y_val.value_counts())\n",
    "print(\"\\nTest set label counts:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# DistilBERT tokenizer 로드\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 데이터 토크나이즈\n",
    "train_encodings = tokenizer(list(X_train_final), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=128)  # 추가된 Validation 데이터\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# 레이블 매핑\n",
    "label_mapping = {'sadness': 0, 'anger': 1, 'happiness': 2}\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = EmotionDataset(train_encodings, list(y_train_final.map(label_mapping)))\n",
    "val_dataset = EmotionDataset(val_encodings, list(y_val.map(label_mapping)))  # 추가된 Validation 데이터셋\n",
    "test_dataset = EmotionDataset(test_encodings, list(y_test.map(label_mapping)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 평가 지표 계산 함수\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# 모델 로드\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 결과 저장 경로\n",
    "    evaluation_strategy=\"epoch\",    # 평가 전략: epoch 단위\n",
    "    save_strategy=\"epoch\",          # 저장 전략: epoch 단위\n",
    "    learning_rate=2e-5,             # 학습률\n",
    "    per_device_train_batch_size=16, # 배치 크기\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,            # 에포크 수를 20으로 수정\n",
    "    weight_decay=0.01,              # 가중치 감쇠\n",
    "    logging_dir='./logs',           # 로깅 경로\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,             # 최대 저장 모델 수\n",
    "    load_best_model_at_end=True,    # 최적 모델 로드\n",
    "    metric_for_best_model=\"accuracy\", # 최적 모델 평가 기준\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   # 학습 데이터셋\n",
    "    eval_dataset=val_dataset,      # 검증 데이터셋 (val_dataset 사용)\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
