{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 토큰을 코드에 직접 설정\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clip-interrogator ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-large...\n",
      "Loading CLIP model ViT-L-14/openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\open_clip\\factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model and data in 2.82 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:376: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "100%|██████████| 55/55 [00:00<00:00, 307.19it/s]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:   0%|          | 0/32 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:  34%|███▍      | 11/32 [00:30<00:57,  2.74s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 128.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 171.39it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 279.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arafed man with a beard and a white shirt is making a face, stock image, rage against the machine, by Helen Thomas Dranga, furious dark haired women, visibly angry, large teeth, frustration, underbite, hyperrealistic portrait, an extremely angry, rage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "# Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))  # 모델 이름을 문자열로 지정\n",
    "\n",
    "# 이미지 경로 설정\n",
    "image_path = \"C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/anger.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# 이미지 처리\n",
    "caption = ci.interrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration\n",
    "from deepface import DeepFace\n",
    "\n",
    "\n",
    "# 2. 감정 분석 파이프라인 초기화 (Hugging Face)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "\n",
    "def classify_emotion(caption):\n",
    "    result = classifier(caption, return_all_scores=True)  # 모든 감정 레이블에 대해 확률을 반환\n",
    "    # 감정 확률 파싱\n",
    "    emotion_probabilities = {item['label'].lower(): round(item['score'], 4) for item in result[0]}\n",
    "    return emotion_probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### choose the mode\n",
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "# 이미지 불러오기\n",
    "image_path = \"C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/anger.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# fast 모드로 Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\", mode=\"fast\"))\n",
    "print(\"\\n=== FAST MODE ===\")\n",
    "print(ci.interrogate(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Interrogator 설정 (Fast 모드 활성화)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m ci \u001b[38;5;241m=\u001b[39m Interrogator(\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-B-32/openai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 이미지 처리 함수 (배치)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image, emotion):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 이미지를 PIL.Image 객체로 변환 (이미 PIL.Image 객체라면 이 단계는 필요하지 않음)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "from transformers import pipeline\n",
    "\n",
    "# Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))  # 모델 이름을 문자열로 지정\n",
    "\n",
    "# 감정 분석 파이프라인 초기화 (Hugging Face)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "\n",
    "# 이미지 경로 설정\n",
    "image_path = \"C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/fear.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# 1. 캡션 생성\n",
    "caption = ci.interrogate(image)\n",
    "\n",
    "# 2. 캡션을 기반으로 감정 분류 함수 (Hugging Face 모든 감정에 대해 확률 반환)\n",
    "def classify_emotion(caption):\n",
    "    result = classifier(caption, return_all_scores=True)  # 모든 감정 레이블에 대해 확률을 반환\n",
    "    # 감정 확률 파싱\n",
    "    emotion_probabilities = {item['label'].lower(): round(item['score'], 4) for item in result[0]}\n",
    "    return emotion_probabilities\n",
    "\n",
    "# 3. 캡션을 기반으로 감정 분석\n",
    "caption_emotion_probabilities = classify_emotion(caption)\n",
    "\n",
    "# 4. 결과 출력 (형식: \"emotion : probability\")\n",
    "print(f\"Generated Caption: {caption}\")\n",
    "print(f\"Caption Emotion Probabilities: {caption_emotion_probabilities}\")\n",
    "\n",
    "# 감정 분석 결과를 내림차순으로 정렬하여 출력\n",
    "sorted_emotions = sorted(caption_emotion_probabilities.items(), key=lambda item: item[1], reverse=True)\n",
    "for emotion, probability in sorted_emotions:\n",
    "    print(f\"{emotion} : {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-large...\n",
      "Loading CLIP model ViT-L-14/openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\open_clip\\factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model and data in 3.88 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:376: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "100%|██████████| 55/55 [00:00<00:00, 245.48it/s]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:   0%|          | 0/32 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:   3%|▎         | 1/32 [00:03<01:37,  3.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232d440fedac4758ad260919ebc859cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flavor chain:  22%|██▏       | 7/32 [00:13<00:45,  1.84s/it]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--michellejieli--emotion_text_classifier. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Flavor chain:  44%|████▍     | 14/32 [00:28<00:36,  2.03s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 287.89it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 214.24it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 302.96it/s]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/01.jpg\n",
      "Generated Caption: arafed male in black sweater sitting in front of a wooden door, kim hyun joo, bixbite, intense look in the eyes, black and aqua colors, by Wang Wu, short dark haircut, pale skin and dark eyes, <hd, secret romance, full - body majestic angel, dark color palette, grim reaper, cocky smirk, a gorgeous\n",
      "joy : 0.8851\n",
      "neutral : 0.1042\n",
      "sadness : 0.0052\n",
      "disgust : 0.0021\n",
      "fear : 0.0013\n",
      "anger : 0.001\n",
      "surprise : 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 297.23it/s]\n",
      "Flavor chain:  34%|███▍      | 11/32 [00:24<00:46,  2.22s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 294.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 181.77it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 187.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/anger.jpg\n",
      "Generated Caption: arafed man with a beard and a white shirt is making a face, stock image, rage against the machine, by Helen Thomas Dranga, furious dark haired women, visibly angry, large long pointy teeth, frustration, full body realistic portrait, showing anger, underbite, emotions\n",
      "anger : 0.9866\n",
      "neutral : 0.0058\n",
      "surprise : 0.0034\n",
      "disgust : 0.0024\n",
      "joy : 0.0009\n",
      "sadness : 0.0005\n",
      "fear : 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 290.94it/s]\n",
      "Flavor chain:  25%|██▌       | 8/32 [00:20<01:01,  2.58s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 294.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 199.95it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 314.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/camera.jpg\n",
      "Generated Caption: woman taking a picture of a train crossing with a camera, song hye - kyo, happily smiling at the camera, in style of atey ghailan, inspired by Byeon Sang-byeok, anya_taylor-joy, happy and smiling\n",
      "joy : 0.9751\n",
      "neutral : 0.0208\n",
      "surprise : 0.0013\n",
      "anger : 0.0009\n",
      "sadness : 0.0008\n",
      "disgust : 0.0006\n",
      "fear : 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [00:29<00:43,  2.30s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 302.13it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 187.45it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 316.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/disgust.jpg\n",
      "Generated Caption: a man with a long nose and a long nose, cartoon, character, man png and psd, sewage falling from grates, floating dust particles, covered in transparent cloth, flat icon, crying and puking, pale bluish skin, lightly dirty face, coughing, profile photo, wojak, very smoky, guy, emoji\n",
      "neutral : 0.5278\n",
      "disgust : 0.3511\n",
      "joy : 0.1021\n",
      "sadness : 0.0077\n",
      "surprise : 0.0072\n",
      "fear : 0.0025\n",
      "anger : 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 292.48it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [00:21<00:55,  2.43s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 308.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 230.71it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 185.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/insta_1.jpg\n",
      "Generated Caption: araffes in a black top and shorts standing in the rain, girl figure, attractive body, carrying guns, wavy long - length black hair, water park, attractive face and body, long elf ears, attractive pose, ready\n",
      "neutral : 0.5822\n",
      "joy : 0.3943\n",
      "disgust : 0.0096\n",
      "surprise : 0.0054\n",
      "sadness : 0.005\n",
      "fear : 0.0021\n",
      "anger : 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 286.39it/s]\n",
      "Flavor chain:  34%|███▍      | 11/32 [00:26<00:50,  2.38s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 305.48it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 222.17it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 301.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/karina.jpg\n",
      "Generated Caption: a close up of a person with a pink sweater and a camera, portrait of female korean idol, waving hands, very sexy woman with black hair, black jacket | shiny, ivory pale skin, medibang, file photo, cutie, waving, frontpage, 0 0 0\n",
      "joy : 0.745\n",
      "neutral : 0.2335\n",
      "surprise : 0.0091\n",
      "disgust : 0.0082\n",
      "sadness : 0.0017\n",
      "anger : 0.0015\n",
      "fear : 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 308.92it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [00:32<00:47,  2.52s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 308.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 193.50it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 316.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/mountain.jpg\n",
      "Generated Caption: mountains with a few clouds in the sky and a few trees, coban, panoramic photography, pexels contest winner, by Dicky Doyle, maui, affinity photo, during sunrise, on top of a mountain, realistic light and shadow, green blue red colors, viewpoint is to front and left, hdr, shadowed\n",
      "neutral : 0.7074\n",
      "joy : 0.2787\n",
      "surprise : 0.0067\n",
      "sadness : 0.0038\n",
      "anger : 0.002\n",
      "fear : 0.0009\n",
      "disgust : 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 314.21it/s]\n",
      "Flavor chain:  25%|██▌       | 8/32 [00:21<01:03,  2.66s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 334.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 206.96it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 312.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/real_test.jpg\n",
      "Generated Caption: arafed woman in a brown dress is dancing in a pose, alessandra ambrosio, inspired by Tani Bunchō, flowing black gown, inspired by Nína Tryggvadóttir, masculine pose, wearing a long flowy fabric, fibonacci flow, exaggerated muscles\n",
      "joy : 0.6265\n",
      "neutral : 0.3526\n",
      "disgust : 0.0098\n",
      "surprise : 0.005\n",
      "sadness : 0.0022\n",
      "fear : 0.0021\n",
      "anger : 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 188.61it/s]\n",
      "Flavor chain:  31%|███▏      | 10/32 [00:23<00:51,  2.36s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 335.29it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 214.24it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 328.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/sad.jpg\n",
      "Generated Caption: arafed baby with a blue bib and a blue bib on it ' s head, anguish, young cute wan asian face, macro image!!!!!, facepalm, aww\n",
      "sadness : 0.5946\n",
      "neutral : 0.2482\n",
      "joy : 0.1249\n",
      "fear : 0.0229\n",
      "disgust : 0.004\n",
      "anger : 0.0032\n",
      "surprise : 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 302.13it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [00:20<00:53,  2.33s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 323.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 214.24it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 322.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/sunset.jpg\n",
      "Generated Caption: there is a lone tree on a hill with a flock of birds flying in the sky, golden hour 4k, widescreen shot, by Ryan Barger, yellow and ornage color scheme, standing in an apple orchard, southern california, featured on artsation, photomanipulation, by Tyler Jacobson\n",
      "neutral : 0.8886\n",
      "joy : 0.0581\n",
      "surprise : 0.0412\n",
      "fear : 0.0082\n",
      "anger : 0.0015\n",
      "disgust : 0.0013\n",
      "sadness : 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 307.19it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [00:21<00:55,  2.41s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 307.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 166.63it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 326.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/th.jpg\n",
      "Generated Caption: araffe kid in a brown coat blowing bubbles with a face mask, news photo, by Kose Kanaoka, made in 2019, a large sphere of red energy, colorful iridescent and playful, bright sunlight, fine bubbles, upbeat, energetic\n",
      "joy : 0.6411\n",
      "neutral : 0.3465\n",
      "surprise : 0.0048\n",
      "anger : 0.0027\n",
      "disgust : 0.0026\n",
      "sadness : 0.0015\n",
      "fear : 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 319.69it/s]\n",
      "Flavor chain:  34%|███▍      | 11/32 [00:25<00:47,  2.28s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 177.38it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 239.95it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 328.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/강의실.jpg\n",
      "Generated Caption: there are many desks in this classroom with a lot of windows, f1.8 anamorphic, by Marie-Suzanne Giroust, photo of a classroom, 2019, commercially ready, photo taken from behind, sunny light, interconnections, 240p, realistic professional photo, by Kathleen Allen\n",
      "neutral : 0.8105\n",
      "joy : 0.1717\n",
      "surprise : 0.0111\n",
      "sadness : 0.0026\n",
      "anger : 0.0024\n",
      "disgust : 0.0009\n",
      "fear : 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 308.92it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [00:29<00:42,  2.23s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 295.63it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 230.71it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 301.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/낮바다.jpg\n",
      "Generated Caption: sunset over the ocean with a sailboat in the foreground, ebay listing thumbnail, israel, an ai generated image, sunsetting color, with clouds in the sky, profile picture 1024px, full width, unedited, florida, amateur photo, maybe small waves, connectedness, by Susan Crile\n",
      "neutral : 0.8149\n",
      "joy : 0.1761\n",
      "surprise : 0.0036\n",
      "sadness : 0.0023\n",
      "anger : 0.0013\n",
      "disgust : 0.0009\n",
      "fear : 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 284.90it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [00:24<01:03,  2.74s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 269.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 176.43it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 290.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/밤바다.jpg\n",
      "Generated Caption: a full moon is shining over the ocean at night, still from a music video, black and aqua colors, silhuette, featured on pexels, with a black dark background, the sea of sadness, connectedness, blue and gray colors, in darkness\n",
      "sadness : 0.8037\n",
      "neutral : 0.1306\n",
      "joy : 0.0593\n",
      "disgust : 0.0025\n",
      "fear : 0.0019\n",
      "anger : 0.001\n",
      "surprise : 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 297.23it/s]\n",
      "Flavor chain:  31%|███▏      | 10/32 [00:22<00:48,  2.21s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 289.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 239.95it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 182.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/악마지성.png\n",
      "Generated Caption: there are two people that are holding up their hands with a picture of them, an affable devil among demons, in a classroom, dark blue color, waving at the camera, japanese facial features, two young men, with pointy ears, turned back to camera, sketch, kind smile\n",
      "neutral : 0.7079\n",
      "joy : 0.1653\n",
      "disgust : 0.1003\n",
      "surprise : 0.017\n",
      "sadness : 0.0046\n",
      "anger : 0.0033\n",
      "fear : 0.0017\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))\n",
    "\n",
    "# 감정 분석 파이프라인 초기화 (Hugging Face)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "\n",
    "# 이미지 경로 목록 설정\n",
    "image_folder = \"C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/\"\n",
    "image_paths = [\n",
    "    os.path.join(image_folder, filename)\n",
    "    for filename in os.listdir(image_folder)[:15]  # 첫 15개 이미지 파일 선택\n",
    "]\n",
    "\n",
    "# 결과 저장할 리스트\n",
    "results = []\n",
    "\n",
    "# 각 이미지에 대해 캡션 및 감정 분석 수행\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # 이미지 로드\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # 1. 캡션 생성\n",
    "        caption = ci.interrogate(image)\n",
    "\n",
    "        # 2. 감정 분석\n",
    "        caption_emotion_probabilities = {\n",
    "            item['label'].lower(): round(item['score'], 4)\n",
    "            for item in classifier(caption, return_all_scores=True)[0]\n",
    "        }\n",
    "\n",
    "        # 3. 결과 저장\n",
    "        result = {\n",
    "            \"image_path\": image_path,\n",
    "            \"caption\": caption,\n",
    "            \"emotion_probabilities\": caption_emotion_probabilities\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        # 출력 결과 확인\n",
    "        print(f\"\\nImage: {image_path}\")\n",
    "        print(f\"Generated Caption: {caption}\")\n",
    "        sorted_emotions = sorted(caption_emotion_probabilities.items(), key=lambda item: item[1], reverse=True)\n",
    "        for emotion, probability in sorted_emotions:\n",
    "            print(f\"{emotion} : {probability}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# 전체 결과를 파일에 저장할 수도 있습니다.\n",
    "import json\n",
    "with open(\"caption_emotion_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_24224\\2597697452.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"full_trained_model.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTWithDropout(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# BERTWithDropout 클래스 정의\n",
    "class BERTWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(BERTWithDropout, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 7)  # 감정 레이블 개수만큼 출력\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        return self.classifier(dropout_output)\n",
    "\n",
    "# 모델 로드 및 장치에 할당\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\"full_trained_model.pth\", map_location=device)\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_24224\\1442337773.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"full_trained_model.pth\").to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-large...\n",
      "Loading CLIP model ViT-L-14/openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\open_clip\\factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "ViT-L-14_openai_artists.safetensors: 100%|██████████| 16.2M/16.2M [00:00<00:00, 69.4MB/s]\n",
      "ViT-L-14_openai_flavors.safetensors: 100%|██████████| 155M/155M [00:01<00:00, 102MB/s]  \n",
      "ViT-L-14_openai_mediums.safetensors: 100%|██████████| 146k/146k [00:00<00:00, 10.1MB/s]\n",
      "ViT-L-14_openai_movements.safetensors: 100%|██████████| 307k/307k [00:00<00:00, 16.1MB/s]\n",
      "ViT-L-14_openai_trendings.safetensors: 100%|██████████| 111k/111k [00:00<00:00, 8.62MB/s]\n",
      "ViT-L-14_openai_negative.safetensors: 100%|██████████| 63.2k/63.2k [00:00<00:00, 21.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model and data in 10.96 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:376: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:   0%|          | 0/32 [00:00<?, ?it/s]c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Flavor chain:  44%|████▍     | 14/32 [02:17<02:56,  9.82s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 297.23it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 206.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 316.38it/s]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\clip_interrogator\\clip_interrogator.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/01.jpg\n",
      "Generated Caption: arafed male in black sweater sitting in front of a wooden door, kim hyun joo, bixbite, intense look in the eyes, black and aqua colors, by Wang Wu, short dark haircut, pale skin and dark eyes, <hd, secret romance, full - body majestic angel, dark color palette, grim reaper, cocky smirk, a gorgeous\n",
      "joy : 0.7753\n",
      "sadness : 0.1485\n",
      "anger : 0.0683\n",
      "neutral : 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 302.13it/s]\n",
      "Flavor chain:  38%|███▊      | 12/32 [01:53<03:09,  9.49s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 193.50it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 304.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/anger.jpg\n",
      "Generated Caption: arafed man with a beard and a white shirt is making a face, stock image, rage against the machine, by Helen Thomas Dranga, furious dark haired women, visibly angry, large teeth, frustration, underbite, hyper realistic portrait, anger, anti-aliased, artist impression\n",
      "anger : 0.985\n",
      "sadness : 0.0055\n",
      "joy : 0.0052\n",
      "neutral : 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "Flavor chain:  25%|██▌       | 8/32 [01:18<03:55,  9.83s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 189.61it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 206.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 322.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/camera.jpg\n",
      "Generated Caption: woman taking a picture of a train crossing with a camera, song hye - kyo, happily smiling at the camera, in style of atey ghailan, inspired by Byeon Sang-byeok, anya_taylor-joy, happy and smiling\n",
      "joy : 0.9554\n",
      "sadness : 0.0308\n",
      "anger : 0.0083\n",
      "neutral : 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 305.48it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [02:02<02:58,  9.39s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 290.94it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 193.51it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 302.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/disgust.jpg\n",
      "Generated Caption: a man with a long nose and a long nose, cartoon, character, man png and psd, sewage falling from grates, floating dust particles, covered in transparent cloth, flat icon, crying and puking, pale bluish skin, lightly dirty face, coughing, profile photo, wojak, very smoky, guy, emoji\n",
      "sadness : 0.6865\n",
      "joy : 0.1663\n",
      "anger : 0.1415\n",
      "neutral : 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 292.49it/s]\n",
      "Flavor chain:  44%|████▍     | 14/32 [02:11<02:48,  9.37s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 290.94it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 222.17it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 318.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/fear.jpg\n",
      "Generated Caption: arafed man in a white shirt and tie standing in front of a shadow of a dragon, attack vector, a stock photo, monster teeth, the victim is in the center, shadows screaming, inhabited on many levels, stock photography, distress, crustacean, angry complexion, black silhouette, by Munch, gigantic monster, pc\n",
      "anger : 0.5541\n",
      "sadness : 0.3255\n",
      "joy : 0.1133\n",
      "neutral : 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 279.12it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [01:27<03:42,  9.69s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 190.40it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 199.96it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 340.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/insta_1.jpg\n",
      "Generated Caption: araffes in a black top and shorts standing in the rain, girl figure, attractive body, carrying guns, wavy long - length black hair, water park, attractive face and body, long elf ears, attractive pose, ready\n",
      "joy : 0.5016\n",
      "sadness : 0.2678\n",
      "anger : 0.1926\n",
      "neutral : 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 308.92it/s]\n",
      "Flavor chain:  44%|████▍     | 14/32 [02:10<02:48,  9.34s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 294.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 206.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 318.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/karina.jpg\n",
      "Generated Caption: a close up of a person with a pink sweater and a camera, portrait of female korean idol, waving hands, very sexy woman with black hair, black jacket | shiny, pale gray skin, medibang, file photo, airport, -n 5, waving, kami, on amino, wearing a cardigan, 2019 trending photo\n",
      "joy : 0.5872\n",
      "sadness : 0.3448\n",
      "anger : 0.0515\n",
      "neutral : 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [02:02<02:58,  9.41s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 300.78it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 222.17it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 328.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/mountain.jpg\n",
      "Generated Caption: mountains with a few clouds in the sky and a few trees, coban, panoramic photography, pexels contest winner, by Dicky Doyle, maui, affinity photo, during sunrise, on top of a mountain, realistic light and shadow, green blue red colors, viewpoint is to front and left, hdr, no fog\n",
      "joy : 0.7428\n",
      "sadness : 0.2097\n",
      "anger : 0.0313\n",
      "neutral : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 298.84it/s]\n",
      "Flavor chain:  25%|██▌       | 8/32 [01:18<03:54,  9.77s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 303.79it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 193.51it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 189.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/party.jpg\n",
      "Generated Caption: group of people standing in a circle with champagne glasses, happy and spirited expression, profile picture 1024px, christmas night, interconnected human lifeforms, playful and cheerful, at a bar, commercial banner, laughter\n",
      "joy : 0.9556\n",
      "sadness : 0.0302\n",
      "anger : 0.0074\n",
      "neutral : 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 327.30it/s]\n",
      "Flavor chain:  25%|██▌       | 8/32 [01:18<03:55,  9.80s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 305.48it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 239.95it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 295.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/real_test.jpg\n",
      "Generated Caption: arafed woman in a brown dress is dancing in a pose, alessandra ambrosio, inspired by Tani Bunchō, flowing black gown, inspired by Nína Tryggvadóttir, masculine pose, wearing a long flowy fabric, old black and white photo, long boney limbs\n",
      "joy : 0.5101\n",
      "sadness : 0.3194\n",
      "anger : 0.1474\n",
      "neutral : 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 300.48it/s]\n",
      "Flavor chain:  31%|███▏      | 10/32 [01:35<03:31,  9.60s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 294.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 214.23it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 318.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/sad.jpg\n",
      "Generated Caption: arafed baby with a blue bib and a blue bib on it ' s head, anguish, young cute wan asian face, macro image!!!!!, facepalm, aww\n",
      "sadness : 0.9509\n",
      "anger : 0.0295\n",
      "joy : 0.0155\n",
      "neutral : 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 297.23it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [01:47<04:35, 11.98s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 310.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 222.17it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 189.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/sunset.jpg\n",
      "Generated Caption: there is a lone tree on a hill with a flock of birds flying in the sky, golden hour 4k, widescreen shot, by Ryan Barger, yellow and ornage color scheme, standing in an apple orchard, southern california, featured on artsation, photomanipulation, by Tyler Jacobson\n",
      "sadness : 0.5292\n",
      "joy : 0.4231\n",
      "anger : 0.0346\n",
      "neutral : 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 248.81it/s]\n",
      "Flavor chain:  28%|██▊       | 9/32 [01:30<03:50, 10.04s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 317.84it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 230.72it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 302.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/th.jpg\n",
      "Generated Caption: araffe kid in a brown coat blowing bubbles with a face mask, news photo, by Kose Kanaoka, made in 2019, a large sphere of red energy, colorful iridescent and playful, bright sunlight, fine bubbles, upbeat, energetic\n",
      "joy : 0.9077\n",
      "sadness : 0.0638\n",
      "anger : 0.0168\n",
      "neutral : 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 300.48it/s]\n",
      "Flavor chain:  34%|███▍      | 11/32 [01:44<03:19,  9.52s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 277.71it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 206.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 308.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/강의실.jpg\n",
      "Generated Caption: there are many desks in this classroom with a lot of windows, f1.8 anamorphic, by Marie-Suzanne Giroust, photo of a classroom, 2019, commercially ready, photo taken from behind, sunny light, interconnections, 240p, realistic professional photo, by Kathleen Allen\n",
      "joy : 0.7542\n",
      "sadness : 0.1796\n",
      "anger : 0.0371\n",
      "neutral : 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 302.13it/s]\n",
      "Flavor chain:  41%|████      | 13/32 [02:02<02:58,  9.39s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 300.48it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 239.94it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 183.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/낮바다.jpg\n",
      "Generated Caption: sunset over the ocean with a sailboat in the foreground, ebay listing thumbnail, israel, an ai generated image, sunsetting color, with clouds in the sky, profile picture 1024px, full width, unedited, florida, amateur photo, maybe small waves, connectedness, by Susan Crile\n",
      "sadness : 0.6042\n",
      "joy : 0.3448\n",
      "anger : 0.0351\n",
      "neutral : 0.0159\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 모델과 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 저장된 감정 분석 모델 로드\n",
    "model = torch.load(\"full_trained_model.pth\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# BERT 토크나이저 로드 (모델과 동일한 BERT 토크나이저 사용)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))\n",
    "\n",
    "# 이미지 경로 목록 설정\n",
    "image_folder = \"C:/Users/user/Desktop/jupyter notebook/gg-project-main/uploads/\"\n",
    "image_paths = [\n",
    "    os.path.join(image_folder, filename)\n",
    "    for filename in os.listdir(image_folder)[:15]  # 첫 15개 이미지 파일 선택\n",
    "]\n",
    "\n",
    "# 결과 저장할 리스트\n",
    "results = []\n",
    "\n",
    "# 감정 레이블 정의\n",
    "emotion_labels = ['anger', 'joy', 'neutral', 'sadness']\n",
    "\n",
    "# 2. 각 이미지에 대해 캡션 생성 및 감정 분석 수행\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # 이미지 로드\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # 1. 캡션 생성\n",
    "        caption = ci.interrogate(image)\n",
    "\n",
    "        # 2. 텍스트 전처리 및 감정 예측\n",
    "        inputs = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "            probabilities = F.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # 예측 결과를 딕셔너리 형태로 정리\n",
    "        caption_emotion_probabilities = {\n",
    "            emotion_labels[i]: round(float(prob), 4) for i, prob in enumerate(probabilities)\n",
    "        }\n",
    "\n",
    "        # 3. 결과 저장\n",
    "        result = {\n",
    "            \"image_path\": image_path,\n",
    "            \"caption\": caption,\n",
    "            \"emotion_probabilities\": caption_emotion_probabilities\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        # 출력 결과 확인\n",
    "        print(f\"\\nImage: {image_path}\")\n",
    "        print(f\"Generated Caption: {caption}\")\n",
    "        sorted_emotions = sorted(caption_emotion_probabilities.items(), key=lambda item: item[1], reverse=True)\n",
    "        for emotion, probability in sorted_emotions:\n",
    "            print(f\"{emotion} : {probability}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# 전체 결과를 파일에 저장할 수도 있습니다.\n",
    "with open(\"caption_emotion_results_mymodel.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
