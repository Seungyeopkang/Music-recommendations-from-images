{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 토큰을 코드에 직접 설정\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c48c9cab5540fd97fab05b95bd0dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02afa2d2fb34c99b4a5369e967e6d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'emotion', 'label', 'image_id'],\n",
      "    num_rows: 3000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 로드\n",
    "ds = load_dataset(\"xodhks/EmoSet118K\")\n",
    "\n",
    "# DatasetDict를 단일 Dataset으로 병합\n",
    "if isinstance(ds, dict):  # DatasetDict인지 확인\n",
    "    # 모든 분할을 병합하는 대신, 필요한 분할을 선택합니다.\n",
    "    dataset = ds['train']  # 'train' 분할을 선택 (필요에 따라 조정)\n",
    "else:\n",
    "    dataset = ds  # 단일 Dataset인 경우 그대로 사용\n",
    "\n",
    "# 감정 필터링: happiness, sadness, anger만 남기기\n",
    "filtered_ds = dataset.filter(lambda example: example['emotion'] in ['anger'])\n",
    "\n",
    "# 각 감정에서 3000개씩 샘플링\n",
    "emotion_counts = {'anger': 3000}\n",
    "sampled_data = []\n",
    "\n",
    "for emotion, count in emotion_counts.items():\n",
    "    # 각 감정별 필터링\n",
    "    emotion_subset = filtered_ds.filter(lambda example: example['emotion'] == emotion)\n",
    "    # 데이터 셔플 및 샘플링\n",
    "    emotion_subset = emotion_subset.shuffle(seed=42)\n",
    "    sampled_subset = emotion_subset.select(range(min(count, len(emotion_subset))))\n",
    "    sampled_data.extend(sampled_subset)\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "from datasets import Dataset\n",
    "final_dataset = Dataset.from_dict({\n",
    "    'image': [example['image'] for example in sampled_data],\n",
    "    'emotion': [example['emotion'] for example in sampled_data],\n",
    "    'label': [example['label'] for example in sampled_data],\n",
    "    'image_id': [example['image_id'] for example in sampled_data],\n",
    "})\n",
    "\n",
    "# 결과 확인\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "print(type(final_dataset[\"image\"][0]))  # 첫 번째 이미지 데이터의 타입 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model ViT-B-32/openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\open_clip\\factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model and data in 2.02 seconds.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip_interrogator import Config, Interrogator\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "subset_dataset = {\n",
    "    'image': final_dataset['image'][400:],\n",
    "    'emotion': final_dataset['emotion'][400:],\n",
    "    'label': final_dataset['label'][400:],\n",
    "    'image_id': final_dataset['image_id'][400:],\n",
    "}\n",
    "\n",
    "# CLIP Interrogator 설정\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-B-32/openai\"))\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 이미지 전처리 파이프라인\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),          # Tensor로 변환\n",
    "])\n",
    "\n",
    "# 데이터셋 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, final_dataset):\n",
    "        self.images = final_dataset[\"image\"]  # 이미지 경로 또는 객체\n",
    "        self.emotions = final_dataset[\"emotion\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # 이미지가 경로(str)인지 확인\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        elif not isinstance(image, Image.Image):\n",
    "            raise TypeError(f\"지원되지 않는 이미지 타입: {type(image)}\")\n",
    "        \n",
    "        image = image_transform(image)  # Tensor로 변환\n",
    "        emotion = self.emotions[idx]\n",
    "        return image, emotion\n",
    "\n",
    "# 배치 처리 함수\n",
    "def process_batch(batch, ci):\n",
    "    images, emotions = batch\n",
    "    results = []\n",
    "    for img, emotion in zip(images, emotions):\n",
    "        img = transforms.ToPILImage()(img)  # Tensor -> PIL 변환\n",
    "        clip_text = ci.interrogate(img)\n",
    "        results.append({\"clip_text\": clip_text, \"emotion\": emotion})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_time = time.time()\\n\\n# 데이터셋 준비\\ndataset = ImageDataset(final_dataset)\\ndataloader = DataLoader(dataset, batch_size=8, num_workers=0, pin_memory=True)\\n\\n# 결과 저장 리스트\\nresults = []\\nfor batch_idx, batch in enumerate(dataloader):\\n    batch_results = process_batch(batch, ci)\\n    results.extend(batch_results)\\n    print(f\"Batch {batch_idx + 1} 완료.\")\\n\\n# 결과를 DataFrame으로 변환\\ndf = pd.DataFrame(results)\\n\\n# CSV 저장\\ncsv_file = \\'clip_results.csv\\'\\nif os.path.exists(csv_file):\\n    # 기존 파일이 있으면 데이터를 추가\\n    df.to_csv(csv_file, mode=\\'a\\', header=False, index=False)\\n    print(f\"기존 \\'{csv_file}\\' 파일에 데이터가 추가되었습니다.\")\\nelse:\\n    # 기존 파일이 없으면 새로 생성\\n    df.to_csv(csv_file, index=False)\\n    print(f\"새로운 \\'{csv_file}\\' 파일이 생성되었습니다.\")\\n\\n# 전체 실행 시간 출력\\nend_time = time.time()\\nprint(f\"전체 실행 시간: {end_time - start_time:.2f}초\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"start_time = time.time()\n",
    "\n",
    "# 데이터셋 준비\n",
    "dataset = ImageDataset(final_dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=8, num_workers=0, pin_memory=True)\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    batch_results = process_batch(batch, ci)\n",
    "    results.extend(batch_results)\n",
    "    print(f\"Batch {batch_idx + 1} 완료.\")\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# CSV 저장\n",
    "csv_file = 'clip_results.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    # 기존 파일이 있으면 데이터를 추가\n",
    "    df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    print(f\"기존 '{csv_file}' 파일에 데이터가 추가되었습니다.\")\n",
    "else:\n",
    "    # 기존 파일이 없으면 새로 생성\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"새로운 '{csv_file}' 파일이 생성되었습니다.\")\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "end_time = time.time()\n",
    "print(f\"전체 실행 시간: {end_time - start_time:.2f}초\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=725x480 at 0x2ECB1B64DF0>, 'emotion': 'happiness', 'label': 0, 'image_id': 'happiness_02342'}\n"
     ]
    }
   ],
   "source": [
    "# final_dataset에서 일부 샘플을 출력하여 image 항목을 확인\n",
    "print(final_dataset[0])  # 첫 번째 항목 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=725x480 at 0x2F2D7E5CF10>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x681 at 0x2F2D7E5C340>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1024 at 0x2F2D7E5CF40>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x532 at 0x2F2D7E5C7F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=856x480 at 0x2EEC26BB310>]\n",
      "['happiness', 'happiness', 'happiness', 'happiness', 'happiness']\n"
     ]
    }
   ],
   "source": [
    "# 'image'와 'emotion' 컬럼의 값 확인\n",
    "print(final_dataset['image'][:5])  # 첫 5개의 이미지 경로\n",
    "print(final_dataset['emotion'][:5])  # 첫 5개의 감정 레이블\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
